{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d22b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, activations, models, preprocessing, utils\n",
    "import re\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60694037",
   "metadata": {},
   "source": [
    "### Criar uma lista com todos os arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3632b7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '../../../dados/portuguese/'\n",
    "files_list = os.listdir(dir_path + os.sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d36f2",
   "metadata": {},
   "source": [
    "### Carregar os arquivos em dois vetores questions e answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16552ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for filepath in files_list:\n",
    "    file = open(dir_path + os.sep + filepath , 'rb')\n",
    "    docs = yaml.safe_load(file)\n",
    "    conversations = docs['conversations']\n",
    "    for con in conversations:\n",
    "        if len(con) > 1 :\n",
    "            questions.append(con[0])\n",
    "            answers.append(con[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e449ca",
   "metadata": {},
   "source": [
    "### Colocar as tag de START e END nas respostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7bb1a40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280, 280)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions), len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766eb7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_tags = []\n",
    "for i in range(len(answers)) :\n",
    "    answers_tags.append('<START> ' + answers[i] + ' <END>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d83da",
   "metadata": {},
   "source": [
    "### Tokenizar e criar o vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "014f9f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(questions + answers_tags)\n",
    "VOCAB_SIZE = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c7a3b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 1,\n",
       " 'end': 2,\n",
       " 'o': 3,\n",
       " 'é': 4,\n",
       " 'de': 5,\n",
       " 'que': 6,\n",
       " 'você': 7,\n",
       " 'e': 8,\n",
       " 'a': 9,\n",
       " 'um': 10,\n",
       " 'do': 11,\n",
       " 'como': 12,\n",
       " 'da': 13,\n",
       " 'em': 14,\n",
       " 'não': 15,\n",
       " 'para': 16,\n",
       " 'bem': 17,\n",
       " 'quem': 18,\n",
       " 'uma': 19,\n",
       " 'os': 20,\n",
       " 'muito': 21,\n",
       " 'eu': 22,\n",
       " 'tem': 23,\n",
       " 'por': 24,\n",
       " 'mais': 25,\n",
       " 'vai': 26,\n",
       " 'qual': 27,\n",
       " 'foi': 28,\n",
       " 'na': 29,\n",
       " 'bom': 30,\n",
       " 'se': 31,\n",
       " 'com': 32,\n",
       " 'me': 33,\n",
       " 'ações': 34,\n",
       " 'ou': 35,\n",
       " 'mercado': 36,\n",
       " 'também': 37,\n",
       " 'seu': 38,\n",
       " 'está': 39,\n",
       " 'estou': 40,\n",
       " 'tudo': 41,\n",
       " 'são': 42,\n",
       " 'obrigado': 43,\n",
       " 'tu': 44,\n",
       " 'minha': 45,\n",
       " 'as': 46,\n",
       " 'no': 47,\n",
       " 'aí': 48,\n",
       " 'dinheiro': 49,\n",
       " 'mas': 50,\n",
       " 'ser': 51,\n",
       " 'gosto': 52,\n",
       " 'melhor': 53,\n",
       " 'acha': 54,\n",
       " 'nome': 55,\n",
       " 'quais': 56,\n",
       " 'quanto': 57,\n",
       " 'linguística': 58,\n",
       " 'faço': 59,\n",
       " 'ele': 60,\n",
       " 'ela': 61,\n",
       " 'sim': 62,\n",
       " 'eae': 63,\n",
       " 'unilab': 64,\n",
       " 'obrigada': 65,\n",
       " 'das': 66,\n",
       " 'és': 67,\n",
       " 'vida': 68,\n",
       " 'sou': 69,\n",
       " 'isso': 70,\n",
       " 'fala': 71,\n",
       " 'te': 72,\n",
       " 'beleza': 73,\n",
       " 'dia': 74,\n",
       " 'quer': 75,\n",
       " 'comprar': 76,\n",
       " 'ai': 77,\n",
       " 'até': 78,\n",
       " 'letras': 79,\n",
       " 'ter': 80,\n",
       " 'curso': 81,\n",
       " 'palavra': 82,\n",
       " 'sua': 83,\n",
       " 'olhos': 84,\n",
       " 'maravilhoso': 85,\n",
       " 'teu': 86,\n",
       " 'faz': 87,\n",
       " 'olá': 88,\n",
       " 'conhece': 89,\n",
       " 'verbo': 90,\n",
       " 'saussure': 91,\n",
       " 'pode': 92,\n",
       " 'dos': 93,\n",
       " 'só': 94,\n",
       " 'dois': 95,\n",
       " 'nunca': 96,\n",
       " 'sei': 97,\n",
       " 'presidente': 98,\n",
       " 'estados': 99,\n",
       " 'primeiro': 100,\n",
       " 'brasil': 101,\n",
       " 'onde': 102,\n",
       " 'há': 103,\n",
       " 'seus': 104,\n",
       " 'amor': 105,\n",
       " 'meu': 106,\n",
       " 'fazer': 107,\n",
       " 'pergunta': 108,\n",
       " 'oi': 109,\n",
       " 'prazer': 110,\n",
       " 'gramática': 111,\n",
       " 'quando': 112,\n",
       " 'porque': 113,\n",
       " 'porquê': 114,\n",
       " 'forma': 115,\n",
       " 'linguagem': 116,\n",
       " 'dólar': 117,\n",
       " 'tanto': 118,\n",
       " 'trabalho': 119,\n",
       " 'ao': 120,\n",
       " 'era': 121,\n",
       " 'pelo': 122,\n",
       " 'pela': 123,\n",
       " 'integração': 124,\n",
       " 'sempre': 125,\n",
       " 'boa': 126,\n",
       " 'ciência': 127,\n",
       " 'linguista': 128,\n",
       " 'língua': 129,\n",
       " 'coloque': 130,\n",
       " 'linda': 131,\n",
       " 'talento': 132,\n",
       " 'luz': 133,\n",
       " 'teus': 134,\n",
       " 'ti': 135,\n",
       " 'tão': 136,\n",
       " 'ótimo': 137,\n",
       " 'hoje': 138,\n",
       " 'jardim': 139,\n",
       " 'meus': 140,\n",
       " 'livro': 141,\n",
       " 'favorito': 142,\n",
       " 'já': 143,\n",
       " 'gosta': 144,\n",
       " 'sonic': 145,\n",
       " 'devo': 146,\n",
       " 'programa': 147,\n",
       " 'chamado': 148,\n",
       " 'pc': 149,\n",
       " 'brother': 150,\n",
       " 'mano': 151,\n",
       " 'amigo': 152,\n",
       " 'estuda': 153,\n",
       " 'substantivo': 154,\n",
       " 'sujeito': 155,\n",
       " 'livros': 156,\n",
       " 'certeza': 157,\n",
       " 'será': 158,\n",
       " 'caso': 159,\n",
       " 'autor': 160,\n",
       " 'análise': 161,\n",
       " 'langue': 162,\n",
       " 'economia': 163,\n",
       " 'ganha': 164,\n",
       " '1': 165,\n",
       " 'casa': 166,\n",
       " 'todos': 167,\n",
       " 'então': 168,\n",
       " 'ferro': 169,\n",
       " 'agora': 170,\n",
       " 'mesmo': 171,\n",
       " 'vou': 172,\n",
       " 'sem': 173,\n",
       " 'tirar': 174,\n",
       " 'fundo': 175,\n",
       " 'copo': 176,\n",
       " 'estado': 177,\n",
       " 'ceará': 178,\n",
       " 'redenção': 179,\n",
       " 'unidos': 180,\n",
       " 'espacial': 181,\n",
       " 'desta': 182,\n",
       " 'significa': 183,\n",
       " 'entre': 184,\n",
       " 'realmente': 185,\n",
       " 'nem': 186,\n",
       " 'tenho': 187,\n",
       " 'quê': 188,\n",
       " 'depende': 189,\n",
       " 'acho': 190,\n",
       " 'ainda': 191,\n",
       " 'frase': 192,\n",
       " 'ferdinand': 193,\n",
       " 'usado': 194,\n",
       " 'recursos': 195,\n",
       " 'etc': 196,\n",
       " 'peixe': 197,\n",
       " 'água': 198,\n",
       " 'sinceridade': 199,\n",
       " 'graça': 200,\n",
       " 'animada': 201,\n",
       " 'insubstituível': 202,\n",
       " 'amo': 203,\n",
       " 'orgulhoso': 204,\n",
       " 'orgulhosa': 205,\n",
       " 'engraçado': 206,\n",
       " 'cama': 207,\n",
       " 'divertido': 208,\n",
       " 'maravilhosa': 209,\n",
       " 'comportamento': 210,\n",
       " 'bonito': 211,\n",
       " 'estas': 212,\n",
       " 'show': 213,\n",
       " 'essa': 214,\n",
       " 'toda': 215,\n",
       " 'bola': 216,\n",
       " 'anjo': 217,\n",
       " 'alma': 218,\n",
       " 'gêmea': 219,\n",
       " 'mulher': 220,\n",
       " 'tua': 221,\n",
       " 'verdade': 222,\n",
       " 'atrai': 223,\n",
       " 'corpo': 224,\n",
       " 'ouviu': 225,\n",
       " 'trabalhando': 226,\n",
       " 'bolo': 227,\n",
       " 'mentira': 228,\n",
       " 'complexo': 229,\n",
       " 'programador': 230,\n",
       " 'sobre': 231,\n",
       " 'artificial': 232,\n",
       " 'alguma': 233,\n",
       " 'linguas': 234,\n",
       " 'conversar': 235,\n",
       " 'resposta': 236,\n",
       " 'algum': 237,\n",
       " 'mario': 238,\n",
       " 'pacman': 239,\n",
       " 'of': 240,\n",
       " '2': 241,\n",
       " 'console': 242,\n",
       " 'saudações': 243,\n",
       " 'la': 244,\n",
       " 'lo': 245,\n",
       " 'tempo': 246,\n",
       " 'tchau': 247,\n",
       " 'noam': 248,\n",
       " 'chomsky': 249,\n",
       " 'marcos': 250,\n",
       " 'bagno': 251,\n",
       " 'todo': 252,\n",
       " 'deve': 253,\n",
       " 'uso': 254,\n",
       " 'figuras': 255,\n",
       " 'pronome': 256,\n",
       " 'anpoll': 257,\n",
       " 'sic': 258,\n",
       " 'pronomes': 259,\n",
       " 'pessoais': 260,\n",
       " 'reto': 261,\n",
       " 'advérbio': 262,\n",
       " 'parole': 263,\n",
       " 'letramento': 264,\n",
       " 'alfabetização': 265,\n",
       " 'taxa': 266,\n",
       " 'estoque': 267,\n",
       " 'conseguiu': 268,\n",
       " 'pai': 269,\n",
       " 'dele': 270,\n",
       " 'cedo': 271,\n",
       " 'faça': 272,\n",
       " 'feito': 273,\n",
       " 'época': 274,\n",
       " 'amanhã': 275,\n",
       " 'nada': 276,\n",
       " 'advogado': 277,\n",
       " 'deu': 278,\n",
       " 'si': 279,\n",
       " 'vencer': 280,\n",
       " 'levar': 281,\n",
       " 'trouxe': 282,\n",
       " 'manchas': 283,\n",
       " 'iogurte': 284,\n",
       " 'resto': 285,\n",
       " 'soro': 286,\n",
       " 'queijo': 287,\n",
       " 'zíper': 288,\n",
       " 'conhecido': 289,\n",
       " 'ano': 290,\n",
       " 'competição': 291,\n",
       " 'fria': 292,\n",
       " 'voo': 293,\n",
       " 'países': 294,\n",
       " 'terra': 295,\n",
       " 'hubble': 296,\n",
       " 'americano': 297,\n",
       " 'grande': 298,\n",
       " 'galáxia': 299,\n",
       " 'the': 300,\n",
       " 'nacional': 301,\n",
       " 'mar': 302,\n",
       " 'jobs': 303,\n",
       " 'homem': 304,\n",
       " 'primeira': 305,\n",
       " 'quantos': 306,\n",
       " 'avião': 307,\n",
       " 'lusofonia': 308,\n",
       " 'localiza': 309,\n",
       " 'cursos': 310,\n",
       " 'turmas': 311,\n",
       " 'anos': 312,\n",
       " 'reitor': 313,\n",
       " 'campus': 314,\n",
       " 'central': 315,\n",
       " 'promover': 316,\n",
       " 'ensino': 317,\n",
       " 'ninguém': 318,\n",
       " 'estudo': 319,\n",
       " 'assim': 320,\n",
       " 'sabe': 321,\n",
       " 'ler': 322,\n",
       " 'meio': 323,\n",
       " 'claro': 324,\n",
       " 'manda': 325,\n",
       " 'papo': 326,\n",
       " 'legal': 327,\n",
       " 'história': 328,\n",
       " 'relacionados': 329,\n",
       " 'à': 330,\n",
       " 'entender': 331,\n",
       " 'ação': 332,\n",
       " 'natureza': 333,\n",
       " 'filósofo': 334,\n",
       " 'regras': 335,\n",
       " 'geralmente': 336,\n",
       " 'essenciais': 337,\n",
       " 'oração': 338,\n",
       " 'universidade': 339,\n",
       " 'outras': 340,\n",
       " 'menos': 341,\n",
       " 'muitas': 342,\n",
       " 'usa': 343,\n",
       " 'valor': 344,\n",
       " 'pra': 345,\n",
       " 'for': 346,\n",
       " 'usados': 347,\n",
       " 'escrita': 348,\n",
       " 'social': 349,\n",
       " 'aprendizagem': 350,\n",
       " 'dar': 351,\n",
       " 'margem': 352,\n",
       " 'necessidade': 353,\n",
       " 'alto': 354,\n",
       " 'on': 355,\n",
       " 'line': 356,\n",
       " 'negociação': 357,\n",
       " 'barato': 358,\n",
       " 'caro': 359,\n",
       " 'trata': 360,\n",
       " 'necessidades': 361,\n",
       " 'mole': 362,\n",
       " 'pedra': 363,\n",
       " 'dura': 364,\n",
       " 'bate': 365,\n",
       " 'fura': 366,\n",
       " 'cão': 367,\n",
       " 'gato': 368,\n",
       " 'caiu': 369,\n",
       " 'vale': 370,\n",
       " '·gua': 371,\n",
       " 'boca': 372,\n",
       " 'grão': 373,\n",
       " 'h·': 374,\n",
       " 'meia': 375,\n",
       " 'coloca': 376,\n",
       " 'deixe': 377,\n",
       " 'agir': 378,\n",
       " 'gado': 379,\n",
       " 'utilize': 380,\n",
       " '2011': 381,\n",
       " 'relatos': 382,\n",
       " 'adoro': 383,\n",
       " 'valente': 384,\n",
       " 'sexy': 385,\n",
       " 'trouxeste': 386,\n",
       " 'expressivos': 387,\n",
       " 'doces': 388,\n",
       " 'ótima': 389,\n",
       " 'lindo': 390,\n",
       " 'divertida': 391,\n",
       " 'inteligente': 392,\n",
       " 'pernas': 393,\n",
       " 'lindas': 394,\n",
       " 'olhar': 395,\n",
       " 'seduz': 396,\n",
       " 'bonita': 397,\n",
       " 'arrumado': 398,\n",
       " 'brilham': 399,\n",
       " 'estrelas': 400,\n",
       " 'admiro': 401,\n",
       " 'nesse': 402,\n",
       " 'vestido': 403,\n",
       " 'tuas': 404,\n",
       " 'palavras': 405,\n",
       " 'confortam': 406,\n",
       " 'deixa': 407,\n",
       " 'nas': 408,\n",
       " 'nuvens': 409,\n",
       " 'apareceu': 410,\n",
       " 'fofa': 411,\n",
       " 'fascina': 412,\n",
       " 'flor': 413,\n",
       " 'faltava': 414,\n",
       " 'sorriso': 415,\n",
       " 'florido': 416,\n",
       " 'pele': 417,\n",
       " 'macia': 418,\n",
       " 'lã': 419,\n",
       " 'meiga': 420,\n",
       " 'rosa': 421,\n",
       " 'atos': 422,\n",
       " 'fascinam': 423,\n",
       " 'perfume': 424,\n",
       " 'tens': 425,\n",
       " 'gostosa': 426,\n",
       " 'lábios': 427,\n",
       " 'mel': 428,\n",
       " 'notícias': 429,\n",
       " 'projeto': 430,\n",
       " 'complicado': 431,\n",
       " 'vsvu': 432,\n",
       " 'dizer': 433,\n",
       " 'viveu': 434,\n",
       " 'posso': 435,\n",
       " 'inteligência': 436,\n",
       " 'opinião': 437,\n",
       " 'comida': 438,\n",
       " 'favorita': 439,\n",
       " 'universo': 440,\n",
       " 'jogo': 441,\n",
       " \"assassin's\": 442,\n",
       " 'creed': 443,\n",
       " 'league': 444,\n",
       " 'legends': 445,\n",
       " 'dota': 446,\n",
       " 'discord': 447,\n",
       " 'team': 448,\n",
       " 'speak': 449,\n",
       " 'skype': 450,\n",
       " 'joga': 451,\n",
       " 'consoles': 452,\n",
       " 'acompanha': 453,\n",
       " 'cenário': 454,\n",
       " 'sports': 455,\n",
       " 'prefere': 456,\n",
       " 'conhecê': 457,\n",
       " 'conhecer': 458,\n",
       " 'suave': 459,\n",
       " 'cara': 460,\n",
       " 'irmão': 461,\n",
       " 'estudante': 462,\n",
       " 'áreas': 463,\n",
       " 'explicar': 464,\n",
       " 'porquês': 465,\n",
       " 'correta': 466,\n",
       " 'concerteza': 467,\n",
       " 'realizado': 468,\n",
       " 'próximo': 469,\n",
       " 'textos': 470,\n",
       " 'comunicação': 471,\n",
       " 'pago': 472,\n",
       " 'juros': 473,\n",
       " 'investimento': 474,\n",
       " 'cobra': 475,\n",
       " 'real': 476,\n",
       " 'dono': 477,\n",
       " 'público': 478,\n",
       " 'médico': 479,\n",
       " 'estão': 480,\n",
       " 'doentes': 481,\n",
       " 'insistiu': 482,\n",
       " 'casar': 483,\n",
       " 'precisei': 484,\n",
       " 'pregar': 485,\n",
       " 'prego': 486,\n",
       " 'tinha': 487,\n",
       " 'martelo': 488,\n",
       " 'preguei': 489,\n",
       " 'pequena': 490,\n",
       " 'barra': 491,\n",
       " 'ganhei': 492,\n",
       " 'quinze': 493,\n",
       " 'queria': 494,\n",
       " 'sagarana': 495,\n",
       " 'sozinha': 496,\n",
       " 'conseguirei': 497,\n",
       " 'concluir': 498,\n",
       " 'estava': 499,\n",
       " 'combinado': 500,\n",
       " 'festa': 501,\n",
       " 'chegou': 502,\n",
       " 'paciente': 503,\n",
       " 'tarde': 504,\n",
       " 'aparecer': 505,\n",
       " 'machocou': 506,\n",
       " 'est·': 507,\n",
       " 'sendo': 508,\n",
       " 'machucado': 509,\n",
       " 'pouco': 510,\n",
       " 'ambicionar': 511,\n",
       " 'perder': 512,\n",
       " 'devagar': 513,\n",
       " 'desacatou': 514,\n",
       " 'permaneci': 515,\n",
       " 'calada': 516,\n",
       " 'depois': 517,\n",
       " 'atropelado': 518,\n",
       " 'atravessa': 519,\n",
       " 'faixa': 520,\n",
       " 'pedestre': 521,\n",
       " 'farol': 522,\n",
       " 'fechado': 523,\n",
       " 'carros': 524,\n",
       " 'fica': 525,\n",
       " 'lembrando': 526,\n",
       " 'piloto': 527,\n",
       " 'continuar': 528,\n",
       " 'pisando': 529,\n",
       " 'tomar': 530,\n",
       " 'providência': 531,\n",
       " 'saber': 532,\n",
       " 'direito': 533,\n",
       " 'chegar': 534,\n",
       " 'evento': 535,\n",
       " 'perguntando': 536,\n",
       " 'achar': 537,\n",
       " 'agência': 538,\n",
       " 'empregos': 539,\n",
       " 'manhã': 540,\n",
       " 'cuide': 541,\n",
       " 'cuido': 542,\n",
       " 'precavida': 543,\n",
       " 'dispensa': 544,\n",
       " 'tornou': 545,\n",
       " 'merece': 546,\n",
       " 'perdão': 547,\n",
       " 'erro': 548,\n",
       " 'moedas': 549,\n",
       " 'juntou': 550,\n",
       " 'cofrinho': 551,\n",
       " 'carro': 552,\n",
       " 'zero': 553,\n",
       " 'comprou': 554,\n",
       " 'impressora': 555,\n",
       " 'barata': 556,\n",
       " 'defeito': 557,\n",
       " 'meses': 558,\n",
       " 'assunto': 559,\n",
       " 'veio': 560,\n",
       " '‡': 561,\n",
       " 'tona': 562,\n",
       " 'acaso': 563,\n",
       " 'pensa': 564,\n",
       " 'antes': 565,\n",
       " 'falar': 566,\n",
       " 'denuncia': 567,\n",
       " 'mesma': 568,\n",
       " '…': 569,\n",
       " 'preciso': 570,\n",
       " 'paciência': 571,\n",
       " 'afirmou': 572,\n",
       " 'precisaria': 573,\n",
       " 'voltar·': 574,\n",
       " 'namorada': 575,\n",
       " 'evitando': 576,\n",
       " 'embora': 577,\n",
       " 'confirme': 578,\n",
       " 'finalmente': 579,\n",
       " 'consegui': 580,\n",
       " 'aquele': 581,\n",
       " 'ingresso': 582,\n",
       " 'esqueci': 583,\n",
       " 'protetor': 584,\n",
       " 'solar': 585,\n",
       " 'emprestou': 586,\n",
       " 'celular': 587,\n",
       " 'propaganda': 588,\n",
       " 'vivo': 589,\n",
       " 'ruim': 590,\n",
       " 'pelos': 591,\n",
       " 'roupa': 592,\n",
       " 'preta': 593,\n",
       " 'gorduras': 594,\n",
       " 'pretas': 595,\n",
       " 'fiquem': 596,\n",
       " 'mofados': 597,\n",
       " 'amarelados': 598,\n",
       " 'secar': 599,\n",
       " 'minhas': 600,\n",
       " 'roupas': 601,\n",
       " 'tempos': 602,\n",
       " 'chuvosos': 603,\n",
       " 'caseiro': 604,\n",
       " 'natural': 605,\n",
       " 'bolsa': 606,\n",
       " 'travando': 607,\n",
       " 'município': 608,\n",
       " 'interior': 609,\n",
       " 'pontos': 610,\n",
       " 'turísticos': 611,\n",
       " 'tais': 612,\n",
       " 'cachoeiras': 613,\n",
       " 'lugares': 614,\n",
       " 'lazer': 615,\n",
       " 'ponto': 616,\n",
       " 'turístico': 617,\n",
       " 'ir': 618,\n",
       " 'dormir': 619,\n",
       " 'trigésimo': 620,\n",
       " 'sétimo': 621,\n",
       " 'john': 622,\n",
       " 'f': 623,\n",
       " 'kennedy': 624,\n",
       " 'assassinado': 625,\n",
       " 'corrida': 626,\n",
       " 'século': 627,\n",
       " '20': 628,\n",
       " 'guerra': 629,\n",
       " 'supremacia': 630,\n",
       " 'eram': 631,\n",
       " 'participaram': 632,\n",
       " 'satélite': 633,\n",
       " 'disco': 634,\n",
       " 'giratório': 635,\n",
       " 'orientação': 636,\n",
       " 'deste': 637,\n",
       " 'eixo': 638,\n",
       " 'afetada': 639,\n",
       " 'inclinação': 640,\n",
       " 'rotação': 641,\n",
       " 'montagem': 642,\n",
       " 'telescópio': 643,\n",
       " 'lançado': 644,\n",
       " 'órbita': 645,\n",
       " 'baixa': 646,\n",
       " '1990': 647,\n",
       " 'nomeado': 648,\n",
       " 'homenagem': 649,\n",
       " 'astrónomo': 650,\n",
       " 'próxima': 651,\n",
       " 'via': 652,\n",
       " 'láctea': 653,\n",
       " 'god': 654,\n",
       " 'save': 655,\n",
       " 'queen': 656,\n",
       " 'hino': 657,\n",
       " 'país': 658,\n",
       " 'shelf': 659,\n",
       " 'celtic': 660,\n",
       " 'sob': 661,\n",
       " 'céltico': 662,\n",
       " 'parte': 663,\n",
       " 'plataforma': 664,\n",
       " 'continental': 665,\n",
       " 'continente': 666,\n",
       " 'golfinhos': 667,\n",
       " 'usam': 668,\n",
       " 'sentido': 669,\n",
       " 'semelhante': 670,\n",
       " 'sonar': 671,\n",
       " 'determinar': 672,\n",
       " 'localização': 673,\n",
       " 'itens': 674,\n",
       " 'próximos': 675,\n",
       " 'steve': 676,\n",
       " 'pisar': 677,\n",
       " 'lua': 678,\n",
       " 'capital': 679,\n",
       " 'iventou': 680,\n",
       " 'mís': 681,\n",
       " 'inaugurada': 682,\n",
       " 'primeiros': 683,\n",
       " 'bhu': 684,\n",
       " 'duração': 685,\n",
       " 'média': 686,\n",
       " 'quantas': 687,\n",
       " 'coodenador': 688,\n",
       " 'atual': 689,\n",
       " 'quanos': 690,\n",
       " 'aqui': 691,\n",
       " 'sã': 692,\n",
       " 'estrutura': 693,\n",
       " 'somente': 694,\n",
       " 'objetivo': 695,\n",
       " 'sistema': 696,\n",
       " 'unila': 697,\n",
       " 'paises': 698,\n",
       " 'compıe': 699,\n",
       " 'capus': 700,\n",
       " 'outra': 701,\n",
       " 'cidade': 702,\n",
       " 'feita': 703,\n",
       " 'seleção': 704,\n",
       " 'estrangeiros': 705,\n",
       " 'acontece': 706,\n",
       " 'saúde': 707,\n",
       " 'completo': 708,\n",
       " 'vi': 709,\n",
       " 'talentoso': 710,\n",
       " 'nosso': 711,\n",
       " 'sinto': 712,\n",
       " 'feliz': 713,\n",
       " 'ouvir': 714,\n",
       " 'fofo': 715,\n",
       " 'fascinada': 716,\n",
       " 'procurava': 717,\n",
       " 'possível': 718,\n",
       " 'causa': 719,\n",
       " 'sensacional': 720,\n",
       " 'colírio': 721,\n",
       " 'amacia': 722,\n",
       " 'verdadeira': 723,\n",
       " 'veem': 724,\n",
       " 'espero': 725,\n",
       " 'fascinante': 726,\n",
       " 'atraído': 727,\n",
       " 'atraente': 728,\n",
       " 'provou': 729,\n",
       " 'atraentes': 730,\n",
       " 'demais': 731,\n",
       " 'notícia': 732,\n",
       " 'senão': 733,\n",
       " 'seguindo': 734,\n",
       " 'função': 735,\n",
       " 'delicioso': 736,\n",
       " 'simples': 737,\n",
       " 'vive': 738,\n",
       " 'vez': 739,\n",
       " 'define': 740,\n",
       " 'vá': 741,\n",
       " 'frente': 742,\n",
       " 'perguntar': 743,\n",
       " 'bot': 744,\n",
       " 'interessante': 745,\n",
       " 'capaz': 746,\n",
       " 'opinar': 747,\n",
       " 'pô': 748,\n",
       " 'valeu': 749,\n",
       " 'difícil': 750,\n",
       " 'várias': 751,\n",
       " 'diversas': 752,\n",
       " 'computador': 753,\n",
       " '42': 754,\n",
       " 'tirando': 755,\n",
       " 'armário': 756,\n",
       " 'meuq': 757,\n",
       " 'gostaria': 758,\n",
       " 'novo': 759,\n",
       " 'prince': 760,\n",
       " 'persia': 761,\n",
       " 'culpa': 762,\n",
       " 'jungler': 763,\n",
       " 'juggernaut': 764,\n",
       " 'lane': 765,\n",
       " 'prefiro': 766,\n",
       " 'gastar': 767,\n",
       " 'master': 768,\n",
       " 'race': 769,\n",
       " 'usei': 770,\n",
       " 'saudades': 771,\n",
       " 'vezes': 772,\n",
       " 'possivel': 773,\n",
       " 'ok': 774,\n",
       " 'poderia': 775,\n",
       " 'estar': 776,\n",
       " 'céu': 777,\n",
       " 'esta': 778,\n",
       " 'acima': 779,\n",
       " 'suavidade': 780,\n",
       " 'bro': 781,\n",
       " 'faaala': 782,\n",
       " 'opaa': 783,\n",
       " 'fenômenos': 784,\n",
       " 'verbal': 785,\n",
       " 'humana': 786,\n",
       " 'buscando': 787,\n",
       " 'características': 788,\n",
       " 'princípios': 789,\n",
       " 'regem': 790,\n",
       " 'às': 791,\n",
       " 'estruturas': 792,\n",
       " 'línguas': 793,\n",
       " 'mundo': 794,\n",
       " 'afirma': 795,\n",
       " 'existência': 796,\n",
       " 'fenômeno': 797,\n",
       " 'exemplo': 798,\n",
       " 'mim': 799,\n",
       " 'nesta': 800,\n",
       " 'gostar': 801,\n",
       " 'maria': 802,\n",
       " 'suíço': 803,\n",
       " 'cujas': 804,\n",
       " 'elaborações': 805,\n",
       " 'teóricas': 806,\n",
       " 'propiciaram': 807,\n",
       " 'desenvolvimento': 808,\n",
       " 'enquanto': 809,\n",
       " 'autônoma': 810,\n",
       " 'sistematização': 811,\n",
       " 'podemos': 812,\n",
       " 'encontrar': 813,\n",
       " 'cujo': 814,\n",
       " 'existe': 815,\n",
       " 'certo': 816,\n",
       " 'denomina': 817,\n",
       " 'avram': 818,\n",
       " 'ativista': 819,\n",
       " 'político': 820,\n",
       " 'estadunidense': 821,\n",
       " 'sintática': 822,\n",
       " 'termos': 823,\n",
       " 'responsável': 824,\n",
       " 'realizar': 825,\n",
       " 'sofrer': 826,\n",
       " 'apesar': 827,\n",
       " 'essencial': 828,\n",
       " 'orações': 829,\n",
       " 'confuso': 830,\n",
       " 'prof': 831,\n",
       " 'dr': 832,\n",
       " 'professor': 833,\n",
       " 'brasília': 834,\n",
       " 'principais': 835,\n",
       " 'referências': 836,\n",
       " 'brasileiras': 837,\n",
       " 'área': 838,\n",
       " 'sociolinguística': 839,\n",
       " 'obras': 840,\n",
       " 'preconceito': 841,\n",
       " 'linguístico': 842,\n",
       " 'eulália': 843,\n",
       " 'indicaria': 844,\n",
       " 'três': 845,\n",
       " 'dicionário': 846,\n",
       " 'geral': 847,\n",
       " 'conhecidas': 848,\n",
       " 'computacional': 849,\n",
       " 'discurso': 850,\n",
       " 'funcionalismo': 851,\n",
       " 'texto': 852,\n",
       " 'aplicada': 853,\n",
       " 'perguntas': 854,\n",
       " 'respondê': 855,\n",
       " 'las': 856,\n",
       " 'final': 857,\n",
       " 'frases': 858,\n",
       " 'faltou': 859,\n",
       " 'aula': 860,\n",
       " 'fazes': 861,\n",
       " 'este': 862,\n",
       " 'tipo': 863,\n",
       " 'professora': 864,\n",
       " 'esse': 865,\n",
       " 'tornar': 866,\n",
       " 'expressiva': 867,\n",
       " 'mensagem': 868,\n",
       " 'transmitida': 869,\n",
       " 'lugar': 870,\n",
       " 'refere': 871,\n",
       " 'acompanho': 872,\n",
       " 'qualificando': 873,\n",
       " 'associação': 874,\n",
       " 'pós': 875,\n",
       " 'graduação': 876,\n",
       " 'pesquisa': 877,\n",
       " 'data': 878,\n",
       " 'definida': 879,\n",
       " 'realização': 880,\n",
       " 'v': 881,\n",
       " 'seminário': 882,\n",
       " 'interdisciplinar': 883,\n",
       " 'ciências': 884,\n",
       " 'nós': 885,\n",
       " 'vós': 886,\n",
       " 'eles': 887,\n",
       " 'elas': 888,\n",
       " 'obra': 889,\n",
       " 'francês': 890,\n",
       " 'dominique': 891,\n",
       " 'maingueneau': 892,\n",
       " 'hum': 893,\n",
       " 'modifica': 894,\n",
       " 'sistemática': 895,\n",
       " 'pertence': 896,\n",
       " 'indivíduos': 897,\n",
       " 'considera': 898,\n",
       " 'individual': 899,\n",
       " 'assistemática': 900,\n",
       " 'diz': 901,\n",
       " 'objeto': 902,\n",
       " 'investigação': 903,\n",
       " 'processo': 904,\n",
       " 'histórica': 905,\n",
       " 'leitura': 906,\n",
       " 'contextos': 907,\n",
       " 'formais': 908,\n",
       " 'informais': 909,\n",
       " 'usos': 910,\n",
       " 'utilitários': 911,\n",
       " 'instituição': 912,\n",
       " 'escolar': 913,\n",
       " 'mediante': 914,\n",
       " 'compreende': 915,\n",
       " 'domínio': 916,\n",
       " 'ativo': 917,\n",
       " 'sistemático': 918,\n",
       " 'habilidades': 919,\n",
       " 'escrever': 920,\n",
       " 'baixo': 921,\n",
       " 'vender': 922,\n",
       " 'investir': 923,\n",
       " 'simplesmente': 924,\n",
       " 'cassino': 925,\n",
       " 'recomendaria': 926,\n",
       " 'prever': 927,\n",
       " 'disse': 928,\n",
       " 'devia': 929,\n",
       " 'dicas': 930,\n",
       " 'fundos': 931,\n",
       " 'mútuos': 932,\n",
       " 'podem': 933,\n",
       " 'melhores': 934,\n",
       " 'seja': 935,\n",
       " 'rico': 936,\n",
       " 'indvidual': 937,\n",
       " 'sozinho': 938,\n",
       " 'banco': 939,\n",
       " 'unidade': 940,\n",
       " 'moeda': 941,\n",
       " 'nos': 942,\n",
       " 'peças': 943,\n",
       " 'padrão': 944,\n",
       " 'ouro': 945,\n",
       " 'prata': 946,\n",
       " 'cobre': 947,\n",
       " 'níquel': 948,\n",
       " 'carimbadas': 949,\n",
       " 'autoridade': 950,\n",
       " 'governamental': 951,\n",
       " 'troca': 952,\n",
       " 'medida': 953,\n",
       " 'qualquer': 954,\n",
       " 'substância': 955,\n",
       " 'artigo': 956,\n",
       " 'notas': 957,\n",
       " 'bancárias': 958,\n",
       " 'cheques': 959,\n",
       " 'volume': 960,\n",
       " 'compre': 961,\n",
       " 'venda': 962,\n",
       " 'produção': 963,\n",
       " 'distribuição': 964,\n",
       " 'consumo': 965,\n",
       " 'riqueza': 966,\n",
       " 'vários': 967,\n",
       " 'problemas': 968,\n",
       " 'finanças': 969,\n",
       " 'tributação': 970,\n",
       " 'tecnicamente': 971,\n",
       " 'alocação': 972,\n",
       " 'condições': 973,\n",
       " 'escassez': 974,\n",
       " 'produzir': 975,\n",
       " 'coisas': 976,\n",
       " 'preencher': 977,\n",
       " 'pessoas': 978,\n",
       " 'subindo': 979,\n",
       " 'estamos': 980,\n",
       " 'falando': 981,\n",
       " 'paga': 982,\n",
       " 'esperando': 983,\n",
       " 'aumento': 984,\n",
       " 'breve': 985,\n",
       " 'trabalhe': 986,\n",
       " 'gratuitamente': 987,\n",
       " 'precisamos': 988,\n",
       " 'bens': 989,\n",
       " 'materiais': 990,\n",
       " 'taxas': 991,\n",
       " 'câmbio': 992,\n",
       " 'queima': 993,\n",
       " 'aproximadamente': 994,\n",
       " '3000': 995,\n",
       " 'mês': 996,\n",
       " 'pagar': 997,\n",
       " 'chiclete': 998,\n",
       " 'acionistas': 999,\n",
       " 'ferreiro': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72c15545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1220"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca3e114",
   "metadata": {},
   "source": [
    "### Transformar as questões em sequencias de numeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d42cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
    "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
    "encoder_input_data = preprocessing.sequence.pad_sequences(tokenized_questions, maxlen=maxlen_questions, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9e343f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adoro você',\n",
       " 'gosto da sua sinceridade',\n",
       " 'você é valente',\n",
       " 'você é muito sexy']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19374a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5f6c9a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[383, 7], [52, 13, 83, 199], [7, 4, 384], [7, 4, 21, 385]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_questions[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7bfde5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[383,   7,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [ 52,  13,  83, 199,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  7,   4, 384,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  7,   4,  21, 385,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15cafd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 28)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f72306",
   "metadata": {},
   "source": [
    "### Transformar as respostas em sequencias de numeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61505eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_answers = tokenizer.texts_to_sequences(answers_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6963ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
    "decoder_input_data = preprocessing.sequence.pad_sequences(tokenized_answers , maxlen=maxlen_answers , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8fca2793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c94a918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 37, 2], [1, 6, 30, 2], [1, 43, 2], [1, 7, 4, 6, 4, 2]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_answers[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9527b1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 47)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a70842",
   "metadata": {},
   "source": [
    "### Fazer os as respostas para o decodificador de saida sem a tag START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7885955",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers , maxlen=maxlen_answers , padding='post')\n",
    "decoder_output_data = utils.to_categorical(padded_answers , VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e061f852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[37, 2], [6, 30, 2], [43, 2], [7, 4, 6, 4, 2]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_answers[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8353c7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_data[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dcd312a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 47, 1220)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_output_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef35fbbd",
   "metadata": {},
   "source": [
    "### Fazer o encoder de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19fdc2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=(maxlen_questions ,))\n",
    "encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM(200 , return_state=True)(encoder_embedding)\n",
    "encoder_states = [ state_h , state_c ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f7cbc",
   "metadata": {},
   "source": [
    "### Fazer o decoder de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb5d6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = tf.keras.layers.Input(shape=(maxlen_answers , ))\n",
    "decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM(200 , return_state=True , return_sequences=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786025f",
   "metadata": {},
   "source": [
    "### Fazer o encoder de saida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e79e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs , _ , _ = decoder_lstm (decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE , activation=tf.keras.activations.softmax) \n",
    "output = decoder_dense (decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81387113",
   "metadata": {},
   "source": [
    "### Construir o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "557ef0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7ea8f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 28)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 47)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 28, 200)              244000    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 47, 200)              244000    ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 200),                320800    ['embedding[0][0]']           \n",
      "                              (None, 200),                                                        \n",
      "                              (None, 200)]                                                        \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               [(None, 47, 200),            320800    ['embedding_1[0][0]',         \n",
      "                              (None, 200),                           'lstm[0][1]',                \n",
      "                              (None, 200)]                           'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 47, 1220)             245220    ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1374820 (5.24 MB)\n",
      "Trainable params: 1374820 (5.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cfe897",
   "metadata": {},
   "source": [
    "### Treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "026c14f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "9/9 [==============================] - 15s 186ms/step - loss: 7.0837\n",
      "Epoch 2/200\n",
      "9/9 [==============================] - 2s 211ms/step - loss: 6.6899\n",
      "Epoch 3/200\n",
      "9/9 [==============================] - 2s 184ms/step - loss: 5.7352\n",
      "Epoch 4/200\n",
      "9/9 [==============================] - 2s 196ms/step - loss: 5.4824\n",
      "Epoch 5/200\n",
      "9/9 [==============================] - 2s 194ms/step - loss: 5.3935\n",
      "Epoch 6/200\n",
      "9/9 [==============================] - 2s 205ms/step - loss: 5.3224\n",
      "Epoch 7/200\n",
      "9/9 [==============================] - 2s 210ms/step - loss: 5.2680\n",
      "Epoch 8/200\n",
      "9/9 [==============================] - 2s 194ms/step - loss: 5.2420\n",
      "Epoch 9/200\n",
      "9/9 [==============================] - 2s 222ms/step - loss: 5.1929\n",
      "Epoch 10/200\n",
      "9/9 [==============================] - 3s 305ms/step - loss: 5.1383\n",
      "Epoch 11/200\n",
      "9/9 [==============================] - 2s 235ms/step - loss: 5.0739\n",
      "Epoch 12/200\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 5.0107\n",
      "Epoch 13/200\n",
      "9/9 [==============================] - 2s 232ms/step - loss: 4.9189\n",
      "Epoch 14/200\n",
      "9/9 [==============================] - 2s 247ms/step - loss: 4.9101\n",
      "Epoch 15/200\n",
      "9/9 [==============================] - 3s 297ms/step - loss: 4.8444\n",
      "Epoch 16/200\n",
      "9/9 [==============================] - 2s 211ms/step - loss: 4.8149\n",
      "Epoch 17/200\n",
      "9/9 [==============================] - 2s 218ms/step - loss: 4.7996\n",
      "Epoch 18/200\n",
      "9/9 [==============================] - 2s 191ms/step - loss: 4.7541\n",
      "Epoch 19/200\n",
      "9/9 [==============================] - 2s 184ms/step - loss: 4.7170\n",
      "Epoch 20/200\n",
      "9/9 [==============================] - 2s 208ms/step - loss: 4.6940\n",
      "Epoch 21/200\n",
      "9/9 [==============================] - 2s 191ms/step - loss: 4.6681\n",
      "Epoch 22/200\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 4.6750\n",
      "Epoch 23/200\n",
      "9/9 [==============================] - 2s 245ms/step - loss: 4.6211\n",
      "Epoch 24/200\n",
      "9/9 [==============================] - 2s 235ms/step - loss: 4.6057\n",
      "Epoch 25/200\n",
      "9/9 [==============================] - 2s 198ms/step - loss: 4.5833\n",
      "Epoch 26/200\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 4.5339\n",
      "Epoch 27/200\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 4.5306\n",
      "Epoch 28/200\n",
      "9/9 [==============================] - 2s 207ms/step - loss: 4.4683\n",
      "Epoch 29/200\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 4.4942\n",
      "Epoch 30/200\n",
      "9/9 [==============================] - 2s 270ms/step - loss: 4.4533\n",
      "Epoch 31/200\n",
      "9/9 [==============================] - 2s 271ms/step - loss: 4.4193\n",
      "Epoch 32/200\n",
      "9/9 [==============================] - 2s 234ms/step - loss: 4.3971\n",
      "Epoch 33/200\n",
      "9/9 [==============================] - 2s 231ms/step - loss: 4.3975\n",
      "Epoch 34/200\n",
      "9/9 [==============================] - 2s 206ms/step - loss: 4.3730\n",
      "Epoch 35/200\n",
      "9/9 [==============================] - 2s 209ms/step - loss: 4.3435\n",
      "Epoch 36/200\n",
      "9/9 [==============================] - 2s 191ms/step - loss: 4.2968\n",
      "Epoch 37/200\n",
      "9/9 [==============================] - 2s 221ms/step - loss: 4.2937\n",
      "Epoch 38/200\n",
      "9/9 [==============================] - 2s 230ms/step - loss: 4.2710\n",
      "Epoch 39/200\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 4.2074\n",
      "Epoch 40/200\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 4.2100\n",
      "Epoch 41/200\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 4.1690\n",
      "Epoch 42/200\n",
      "9/9 [==============================] - 2s 176ms/step - loss: 4.1511\n",
      "Epoch 43/200\n",
      "9/9 [==============================] - 2s 182ms/step - loss: 4.1230\n",
      "Epoch 44/200\n",
      "9/9 [==============================] - 2s 182ms/step - loss: 4.1150\n",
      "Epoch 45/200\n",
      "9/9 [==============================] - 2s 217ms/step - loss: 4.0660\n",
      "Epoch 46/200\n",
      "9/9 [==============================] - 2s 211ms/step - loss: 4.0414\n",
      "Epoch 47/200\n",
      "9/9 [==============================] - 2s 210ms/step - loss: 4.0321\n",
      "Epoch 48/200\n",
      "9/9 [==============================] - 2s 211ms/step - loss: 4.0012\n",
      "Epoch 49/200\n",
      "9/9 [==============================] - 2s 191ms/step - loss: 3.9671\n",
      "Epoch 50/200\n",
      "9/9 [==============================] - 2s 193ms/step - loss: 3.9532\n",
      "Epoch 51/200\n",
      "9/9 [==============================] - 2s 193ms/step - loss: 3.9260\n",
      "Epoch 52/200\n",
      "9/9 [==============================] - 2s 185ms/step - loss: 3.8977\n",
      "Epoch 53/200\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 3.8711\n",
      "Epoch 54/200\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 3.8581\n",
      "Epoch 55/200\n",
      "9/9 [==============================] - 2s 191ms/step - loss: 3.8217\n",
      "Epoch 56/200\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 3.7873\n",
      "Epoch 57/200\n",
      "9/9 [==============================] - 2s 203ms/step - loss: 3.7827\n",
      "Epoch 58/200\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 3.7235\n",
      "Epoch 59/200\n",
      "9/9 [==============================] - 2s 209ms/step - loss: 3.7288\n",
      "Epoch 60/200\n",
      "9/9 [==============================] - 2s 204ms/step - loss: 3.6615\n",
      "Epoch 61/200\n",
      "9/9 [==============================] - 2s 221ms/step - loss: 3.6785\n",
      "Epoch 62/200\n",
      "9/9 [==============================] - 2s 206ms/step - loss: 3.6472\n",
      "Epoch 63/200\n",
      "9/9 [==============================] - 2s 192ms/step - loss: 3.6119\n",
      "Epoch 64/200\n",
      "9/9 [==============================] - 2s 203ms/step - loss: 3.5932\n",
      "Epoch 65/200\n",
      "9/9 [==============================] - 2s 211ms/step - loss: 3.5787\n",
      "Epoch 66/200\n",
      "9/9 [==============================] - 2s 217ms/step - loss: 3.5403\n",
      "Epoch 67/200\n",
      "9/9 [==============================] - 2s 209ms/step - loss: 3.5230\n",
      "Epoch 68/200\n",
      "9/9 [==============================] - 2s 219ms/step - loss: 3.5093\n",
      "Epoch 69/200\n",
      "9/9 [==============================] - 2s 225ms/step - loss: 3.4875\n",
      "Epoch 70/200\n",
      "9/9 [==============================] - 2s 220ms/step - loss: 3.4334\n",
      "Epoch 71/200\n",
      "9/9 [==============================] - 2s 224ms/step - loss: 3.4305\n",
      "Epoch 72/200\n",
      "9/9 [==============================] - 2s 219ms/step - loss: 3.4179\n",
      "Epoch 73/200\n",
      "9/9 [==============================] - 2s 214ms/step - loss: 3.3762\n",
      "Epoch 74/200\n",
      "9/9 [==============================] - 2s 220ms/step - loss: 3.3558\n",
      "Epoch 75/200\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 3.3369\n",
      "Epoch 76/200\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 3.3171\n",
      "Epoch 77/200\n",
      "9/9 [==============================] - 2s 181ms/step - loss: 3.2854\n",
      "Epoch 78/200\n",
      "9/9 [==============================] - 2s 178ms/step - loss: 3.2848\n",
      "Epoch 79/200\n",
      "9/9 [==============================] - 2s 184ms/step - loss: 3.2733\n",
      "Epoch 80/200\n",
      "9/9 [==============================] - 2s 207ms/step - loss: 3.2347\n",
      "Epoch 81/200\n",
      "9/9 [==============================] - 2s 225ms/step - loss: 3.1980\n",
      "Epoch 82/200\n",
      "9/9 [==============================] - 2s 211ms/step - loss: 3.1884\n",
      "Epoch 83/200\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 3.1671\n",
      "Epoch 84/200\n",
      "9/9 [==============================] - 2s 214ms/step - loss: 3.1573\n",
      "Epoch 85/200\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 3.1177\n",
      "Epoch 86/200\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 3.0934\n",
      "Epoch 87/200\n",
      "9/9 [==============================] - 2s 208ms/step - loss: 3.0589\n",
      "Epoch 88/200\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 3.0580\n",
      "Epoch 89/200\n",
      "9/9 [==============================] - 2s 215ms/step - loss: 3.0546\n",
      "Epoch 90/200\n",
      "9/9 [==============================] - 2s 207ms/step - loss: 3.0244\n",
      "Epoch 91/200\n",
      "9/9 [==============================] - 2s 206ms/step - loss: 2.9970\n",
      "Epoch 92/200\n",
      "9/9 [==============================] - 2s 204ms/step - loss: 2.9562\n",
      "Epoch 93/200\n",
      "9/9 [==============================] - 2s 226ms/step - loss: 2.9677\n",
      "Epoch 94/200\n",
      "9/9 [==============================] - 2s 232ms/step - loss: 2.9359\n",
      "Epoch 95/200\n",
      "9/9 [==============================] - 2s 203ms/step - loss: 2.9340\n",
      "Epoch 96/200\n",
      "9/9 [==============================] - 2s 208ms/step - loss: 2.8984\n",
      "Epoch 97/200\n",
      "9/9 [==============================] - 2s 191ms/step - loss: 2.8327\n",
      "Epoch 98/200\n",
      "9/9 [==============================] - 2s 193ms/step - loss: 2.8464\n",
      "Epoch 99/200\n",
      "9/9 [==============================] - 2s 193ms/step - loss: 2.8460\n",
      "Epoch 100/200\n",
      "9/9 [==============================] - 2s 189ms/step - loss: 2.8095\n",
      "Epoch 101/200\n",
      "9/9 [==============================] - 2s 191ms/step - loss: 2.7693\n",
      "Epoch 102/200\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 2.7559\n",
      "Epoch 103/200\n",
      "9/9 [==============================] - 2s 186ms/step - loss: 2.7325\n",
      "Epoch 104/200\n",
      "9/9 [==============================] - 2s 207ms/step - loss: 2.7381\n",
      "Epoch 105/200\n",
      "9/9 [==============================] - 2s 215ms/step - loss: 2.7024\n",
      "Epoch 106/200\n",
      "9/9 [==============================] - 2s 193ms/step - loss: 2.7027\n",
      "Epoch 107/200\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 2.6705\n",
      "Epoch 108/200\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 2.6383\n",
      "Epoch 109/200\n",
      "9/9 [==============================] - 2s 206ms/step - loss: 2.6342\n",
      "Epoch 110/200\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 2.5886\n",
      "Epoch 111/200\n",
      "9/9 [==============================] - 2s 223ms/step - loss: 2.5804\n",
      "Epoch 112/200\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 2.5301\n",
      "Epoch 113/200\n",
      "9/9 [==============================] - 2s 205ms/step - loss: 2.5585\n",
      "Epoch 114/200\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 2.5237\n",
      "Epoch 115/200\n",
      "9/9 [==============================] - 2s 193ms/step - loss: 2.4829\n",
      "Epoch 116/200\n",
      "9/9 [==============================] - 2s 188ms/step - loss: 2.4894\n",
      "Epoch 117/200\n",
      "9/9 [==============================] - 2s 190ms/step - loss: 2.4589\n",
      "Epoch 118/200\n",
      "9/9 [==============================] - 2s 228ms/step - loss: 2.4393\n",
      "Epoch 119/200\n",
      "9/9 [==============================] - 2s 194ms/step - loss: 2.4296\n",
      "Epoch 120/200\n",
      "9/9 [==============================] - 2s 203ms/step - loss: 2.3945\n",
      "Epoch 121/200\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 2.3794\n",
      "Epoch 122/200\n",
      "9/9 [==============================] - 2s 205ms/step - loss: 2.3359\n",
      "Epoch 123/200\n",
      "9/9 [==============================] - 2s 204ms/step - loss: 2.3509\n",
      "Epoch 124/200\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 2.3188\n",
      "Epoch 125/200\n",
      "9/9 [==============================] - 2s 203ms/step - loss: 2.3200\n",
      "Epoch 126/200\n",
      "9/9 [==============================] - 2s 206ms/step - loss: 2.2569\n",
      "Epoch 127/200\n",
      "9/9 [==============================] - 2s 215ms/step - loss: 2.2411\n",
      "Epoch 128/200\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 2.2736\n",
      "Epoch 129/200\n",
      "9/9 [==============================] - 2s 218ms/step - loss: 2.2184\n",
      "Epoch 130/200\n",
      "9/9 [==============================] - 2s 205ms/step - loss: 2.2027\n",
      "Epoch 131/200\n",
      "9/9 [==============================] - 2s 187ms/step - loss: 2.1677\n",
      "Epoch 132/200\n",
      "9/9 [==============================] - 2s 205ms/step - loss: 2.1779\n",
      "Epoch 133/200\n",
      "9/9 [==============================] - 2s 193ms/step - loss: 2.1552\n",
      "Epoch 134/200\n",
      "9/9 [==============================] - 2s 170ms/step - loss: 2.1137\n",
      "Epoch 135/200\n",
      "9/9 [==============================] - 2s 174ms/step - loss: 2.0962\n",
      "Epoch 136/200\n",
      "9/9 [==============================] - 2s 188ms/step - loss: 2.0816\n",
      "Epoch 137/200\n",
      "9/9 [==============================] - 2s 216ms/step - loss: 2.0506\n",
      "Epoch 138/200\n",
      "9/9 [==============================] - 2s 193ms/step - loss: 2.0369\n",
      "Epoch 139/200\n",
      "9/9 [==============================] - 2s 193ms/step - loss: 1.9984\n",
      "Epoch 140/200\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 2.0063\n",
      "Epoch 141/200\n",
      "9/9 [==============================] - 2s 187ms/step - loss: 1.9921\n",
      "Epoch 142/200\n",
      "9/9 [==============================] - 2s 192ms/step - loss: 1.9643\n",
      "Epoch 143/200\n",
      "9/9 [==============================] - 2s 178ms/step - loss: 1.9391\n",
      "Epoch 144/200\n",
      "9/9 [==============================] - 2s 193ms/step - loss: 1.9210\n",
      "Epoch 145/200\n",
      "9/9 [==============================] - 2s 195ms/step - loss: 1.8922\n",
      "Epoch 146/200\n",
      "9/9 [==============================] - 2s 179ms/step - loss: 1.8845\n",
      "Epoch 147/200\n",
      "9/9 [==============================] - 2s 178ms/step - loss: 1.8591\n",
      "Epoch 148/200\n",
      "9/9 [==============================] - 2s 181ms/step - loss: 1.8170\n",
      "Epoch 149/200\n",
      "9/9 [==============================] - 2s 176ms/step - loss: 1.8390\n",
      "Epoch 150/200\n",
      "9/9 [==============================] - 2s 191ms/step - loss: 1.7803\n",
      "Epoch 151/200\n",
      "9/9 [==============================] - 2s 183ms/step - loss: 1.7791\n",
      "Epoch 152/200\n",
      "9/9 [==============================] - 2s 182ms/step - loss: 1.7495\n",
      "Epoch 153/200\n",
      "9/9 [==============================] - 2s 178ms/step - loss: 1.7393\n",
      "Epoch 154/200\n",
      "9/9 [==============================] - 2s 180ms/step - loss: 1.7546\n",
      "Epoch 155/200\n",
      "9/9 [==============================] - 2s 174ms/step - loss: 1.6906\n",
      "Epoch 156/200\n",
      "9/9 [==============================] - 2s 187ms/step - loss: 1.6956\n",
      "Epoch 157/200\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 1.6676\n",
      "Epoch 158/200\n",
      "9/9 [==============================] - 2s 199ms/step - loss: 1.6579\n",
      "Epoch 159/200\n",
      "9/9 [==============================] - 2s 200ms/step - loss: 1.6266\n",
      "Epoch 160/200\n",
      "9/9 [==============================] - 2s 186ms/step - loss: 1.5957\n",
      "Epoch 161/200\n",
      "9/9 [==============================] - 2s 186ms/step - loss: 1.5920\n",
      "Epoch 162/200\n",
      "9/9 [==============================] - 2s 179ms/step - loss: 1.5764\n",
      "Epoch 163/200\n",
      "9/9 [==============================] - 2s 184ms/step - loss: 1.5434\n",
      "Epoch 164/200\n",
      "9/9 [==============================] - 2s 191ms/step - loss: 1.5343\n",
      "Epoch 165/200\n",
      "9/9 [==============================] - 2s 205ms/step - loss: 1.5151\n",
      "Epoch 166/200\n",
      "9/9 [==============================] - 2s 183ms/step - loss: 1.5030\n",
      "Epoch 167/200\n",
      "9/9 [==============================] - 2s 170ms/step - loss: 1.4702\n",
      "Epoch 168/200\n",
      "9/9 [==============================] - 2s 172ms/step - loss: 1.4540\n",
      "Epoch 169/200\n",
      "9/9 [==============================] - 2s 186ms/step - loss: 1.4474\n",
      "Epoch 170/200\n",
      "9/9 [==============================] - 2s 186ms/step - loss: 1.4142\n",
      "Epoch 171/200\n",
      "9/9 [==============================] - 2s 187ms/step - loss: 1.4074\n",
      "Epoch 172/200\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 1.3818\n",
      "Epoch 173/200\n",
      "9/9 [==============================] - 2s 201ms/step - loss: 1.3702\n",
      "Epoch 174/200\n",
      "9/9 [==============================] - 2s 187ms/step - loss: 1.3409\n",
      "Epoch 175/200\n",
      "9/9 [==============================] - 2s 197ms/step - loss: 1.3364\n",
      "Epoch 176/200\n",
      "9/9 [==============================] - 2s 215ms/step - loss: 1.3108\n",
      "Epoch 177/200\n",
      "9/9 [==============================] - 2s 207ms/step - loss: 1.2864\n",
      "Epoch 178/200\n",
      "9/9 [==============================] - 3s 301ms/step - loss: 1.2750\n",
      "Epoch 179/200\n",
      "9/9 [==============================] - 2s 262ms/step - loss: 1.2504\n",
      "Epoch 180/200\n",
      "9/9 [==============================] - 2s 252ms/step - loss: 1.2313\n",
      "Epoch 181/200\n",
      "9/9 [==============================] - 3s 292ms/step - loss: 1.2369\n",
      "Epoch 182/200\n",
      "9/9 [==============================] - 3s 338ms/step - loss: 1.2142\n",
      "Epoch 183/200\n",
      "9/9 [==============================] - 4s 399ms/step - loss: 1.1977\n",
      "Epoch 184/200\n",
      "9/9 [==============================] - 2s 252ms/step - loss: 1.1546\n",
      "Epoch 185/200\n",
      "9/9 [==============================] - 3s 294ms/step - loss: 1.1706\n",
      "Epoch 186/200\n",
      "9/9 [==============================] - 2s 226ms/step - loss: 1.1296\n",
      "Epoch 187/200\n",
      "9/9 [==============================] - 3s 343ms/step - loss: 1.1213\n",
      "Epoch 188/200\n",
      "9/9 [==============================] - 4s 452ms/step - loss: 1.1080\n",
      "Epoch 189/200\n",
      "9/9 [==============================] - 4s 437ms/step - loss: 1.0873\n",
      "Epoch 190/200\n",
      "9/9 [==============================] - 3s 301ms/step - loss: 1.0828\n",
      "Epoch 191/200\n",
      "9/9 [==============================] - 3s 273ms/step - loss: 1.0659\n",
      "Epoch 192/200\n",
      "9/9 [==============================] - 3s 317ms/step - loss: 1.0366\n",
      "Epoch 193/200\n",
      "9/9 [==============================] - 3s 297ms/step - loss: 1.0373\n",
      "Epoch 194/200\n",
      "9/9 [==============================] - 2s 254ms/step - loss: 1.0110\n",
      "Epoch 195/200\n",
      "9/9 [==============================] - 2s 223ms/step - loss: 1.0090\n",
      "Epoch 196/200\n",
      "9/9 [==============================] - 2s 221ms/step - loss: 0.9859\n",
      "Epoch 197/200\n",
      "9/9 [==============================] - 2s 275ms/step - loss: 0.9729\n",
      "Epoch 198/200\n",
      "9/9 [==============================] - 2s 275ms/step - loss: 0.9587\n",
      "Epoch 199/200\n",
      "9/9 [==============================] - 2s 256ms/step - loss: 0.9532\n",
      "Epoch 200/200\n",
      "9/9 [==============================] - 2s 215ms/step - loss: 0.9146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1fb56544e50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=32, epochs=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc6ee662",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "decoder_state_input_h = tf.keras.layers.Input(shape=(200 ,))\n",
    "decoder_state_input_c = tf.keras.layers.Input(shape=(200 ,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding , initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = tf.keras.models.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8021c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_sentence):\n",
    "    tokens = input_sentence.lower().split()\n",
    "    tokens_list = []\n",
    "    for word in tokens:\n",
    "        tokens_list.append(tokenizer.word_index[word]) \n",
    "    return preprocessing.sequence.pad_sequences([tokens_list] , maxlen=maxlen_questions , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ea599ccd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'batida'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mword_index[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatida\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'batida'"
     ]
    }
   ],
   "source": [
    "tokenizer.word_index['batida']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3c1046ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = ['oi', 'quem é você', 'qual seu livro favorito', 'você é um humano ou computador', 'o que é linguística', 'o que é uma opinião', 'até mais']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e87b04e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Human: oi\n",
      "\n",
      "Bot:  olá\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Human: quem é você\n",
      "\n",
      "Bot:  eu estou bem e você\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "Human: qual seu livro favorito\n",
      "\n",
      "Bot:  não sei\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Human: você é um humano ou computador\n",
      "\n",
      "Bot:  que bom\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Human: o que é linguística\n",
      "\n",
      "Bot:  tecnicamente é o estudo é a ciência que estuda os fenômenos relacionados de texto linguística de escassez\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Human: o que é uma opinião\n",
      "\n",
      "Bot:  bem a alfabetização pode dar a parole como individual e uma escrita em um nome é um palavra que denomina o nome é o nome é o nome é um palavra que denomina o nome\n",
      "-------------------------\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Human: até mais\n",
      "\n",
      "Bot:  oi\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, frase in enumerate(tests):\n",
    "    frase_processada = preprocess(tests[i])\n",
    "    states_values = encoder_model.predict(frase_processada)\n",
    "    empty_target_seq = np.zeros((1 , 1))\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    \n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = decoder_model.predict([empty_target_seq] + states_values)\n",
    "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
    "        sampled_word = None\n",
    "        \n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += f' {word}'\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros((1 , 1))  \n",
    "        empty_target_seq[0 , 0] = sampled_word_index\n",
    "        states_values = [h , c] \n",
    "    print(f'Human: {tests[i]}')\n",
    "    print()\n",
    "    \n",
    "    decoded_translation = decoded_translation.split(' end')[0]\n",
    "    print(f'Bot: {decoded_translation}')\n",
    "    print('-'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42c020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
